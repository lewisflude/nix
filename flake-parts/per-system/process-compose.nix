{ inputs, ... }:
{
  perSystem = _: {
    # Development services using services-flake
    # Run with: nix run .#dev
    process-compose."dev" = {
      imports = [
        inputs.services-flake.processComposeModules.default
      ];

      services = {
        # PostgreSQL - Most common database for development
        postgres."pg1" = {
          enable = true;
          initialDatabases = [
            { name = "dev"; }
            { name = "test"; }
          ];
          # Persist data in your home directory
          dataDir = "$HOME/.services-flake/postgres";
          # Optional: Create initial script
          # initialScript.before = "CREATE EXTENSION IF NOT EXISTS postgis;";
        };

        # Redis - For caching, sessions, queues
        redis."redis1" = {
          enable = true;
          # Persist data
          dataDir = "$HOME/.services-flake/redis";
        };

        # Ollama - LLM backend for local development
        # Separate from your system Ollama service
        ollama."ollama-dev" = {
          enable = true;
          dataDir = "$HOME/.services-flake/ollama";
          # Models to auto-download (add what you need)
          models = [
            "llama3.2"
            "qwen2.5-coder:7b"
          ];
        };

        # Open WebUI - ChatGPT-like UI for local LLMs
        # Useful for testing AI features
        open-webui."webui-dev" = {
          enable = true;
          environment = {
            # Ollama default port is 11434
            OLLAMA_API_BASE_URL = "http://127.0.0.1:11434";
            # Disable auth for local dev (enable for production!)
            WEBUI_AUTH = "False";
          };
        };

        # MinIO - S3-compatible storage for local testing
        # Uncomment to enable:
        # minio."minio1" = {
        #   enable = true;
        #   # Access via http://localhost:9000
        #   # Console via http://localhost:9001
        #   # Default credentials: minioadmin / minioadmin
        # };

        # Nginx - Local reverse proxy
        # Uncomment to enable:
        # nginx."nginx1" = {
        #   enable = true;
        #   port = 8080;
        # };

        # Grafana + Prometheus for monitoring
        # Uncomment to enable:
        # prometheus."prom1" = {
        #   enable = true;
        # };
        # grafana."grafana1" = {
        #   enable = true;
        #   datasources = [{
        #     name = "Prometheus";
        #     type = "prometheus";
        #     url = "http://${config.services.prometheus.prom1.host}:${toString config.services.prometheus.prom1.port}";
        #   }];
        # };
      };

      # Service dependencies
      # Note: Process names are auto-generated by services-flake
      # Once running, you can see them with: nix run .#dev
      # settings.processes = {
      #   # Wait for Ollama models to download before starting Open WebUI
      #   # open-webui-webui-dev.depends_on.ollama-dev-models.condition = "process_completed_successfully";
      # };
    };
  };
}
